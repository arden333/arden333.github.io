---
layout: single
title: "[ML] 서포트 벡터 머신(SVM)"
categories: "ML"
tags: "SVM"

toc: true
toc_sticky: true
---

서포트 벡터 머신(SVM)은 분류와 회귀 문제를 해결하는데 사용되는 지도학습 알고리즘이다. SVM은 주로 분류 문제에서 사용되며, 특히 이진 분류에 매우 효과적이다.

## SVM의 주요 개념
- 결정경계(Decision Boundary): SVM의 기본 아이디어는 주어진 데이터를 가장 잘 구분할 수 있는 최적의 경계선(결정경계)를 찾는 것이다.
- 마진(Margin): 결정경계와 두 클래스 데이터 사이읙 거리를 의미한다. SVM은 마진을 최대화하는 결정 경계를 찾는 것을 목표로 한다. 즉, 두 클래스 사이의 데이터 포인트에서 가능한 한 가장 먼 경계선을 찾는다.
- 서포트 벡터(Support Vectors): 결정경계를 정의하는데 중요한 역할을 하는 데이터포인트들이다. 마진에 가장 가까운 포인트들을 서포트 벡터라고 한다.

## SVM의 작동원리
SVM은 데이터를 구분하는 여러 가능한 경게선 중에서 마진을 최대화하는 경계선을 찾는다. 만약 두 클래스가 선형적(직선)으로 분리될 수 있다면 SVM은 가장 단순한 직선을 찾아 분류한다. 그러나 실제 데이터는 선형적으로 분리되지 않는 경우가 많다. 이 때 SVM은 커널함수를 사용해 데이터를 고차원 공간으로 변환한다. 이를 통해 선형적으로 데이터를 분리할 수 있게 된다.

## 장단점
**장점**
- SVM은 분류 문제에서 정확도가 높다.
- 고차원 데이터에서도 잘 작동하며 특히 차원의 저주 문제에 잘 대응한다.

**단점**
- 데이터가 많을 경우, 학습에 시간이 오래 걸릴 수 있다.
- 커널함수나 매개변수의 설정에 따라 성능이 달라질 수 있어 최적의 값을 찾는 것이 어렵다.


```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# 데이터셋 로드 (Iris 데이터셋)
iris = datasets.load_iris()
X = iris.data  # 특성(꽃잎 길이, 너비 등)
y = iris.target  # 클래스(0: Setosa, 1: Versicolor, 2: Virginica)

# Setosa와 Versicolor 두 클래스만 사용 (이진 분류)
X = X[y != 2]
y = y[y != 2]

# 학습용 데이터와 테스트용 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# SVM 분류기 생성 (RBF 커널 사용)
  # kernel: 선형 커널
  # C: 학습 오류에 대한 패널티, C 값이 클 수록 모델이 학습 데이터에 좀 더 최적화 됨, 너무 크면 오버피팅 발생
  # gamma: 결정경계를 얼마나 유연하게 그을 것인지 결정. gamma를 높이면 결정경계가 구불구불해짐
  # Epsilon: 임계값, 예측한 값이 GT 범위 안에 있으면 패널티 부여 X
clf = SVC(kernel='rbf', C=1.0, gamma='scale')

# 모델 학습
clf.fit(X_train, y_train)

# 예측
y_pred = clf.predict(X_test)

# 정확도 출력
accuracy = accuracy_score(y_test, y_pred)
print(f"SVM 모델의 정확도: {accuracy * 100:.2f}%")
```

    SVM 모델의 정확도: 100.00%

